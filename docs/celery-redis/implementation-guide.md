# Redis + RQ êµ¬í˜„ ê°€ì´ë“œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd back-end
pip install redis rq rq-dashboard boto3
```

### 2. Redis ì„¤ì¹˜ ë° ì‹¤í–‰

#### macOS (Homebrew)
```bash
brew install redis
brew services start redis
```

#### Docker
```bash
docker run -d -p 6379:6379 --name redis redis:7-alpine
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì— ì¶”ê°€:
```bash
# Redis ì„¤ì •
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# SQS ì„¤ì •
SQS_QUEUE_URL=https://sqs.ap-northeast-2.amazonaws.com/YOUR_ACCOUNT/audio-analysis-queue
SQS_RESULT_QUEUE_URL=https://sqs.ap-northeast-2.amazonaws.com/YOUR_ACCOUNT/analysis-results-queue

# í ì‹œìŠ¤í…œ í™œì„±í™”
USE_QUEUE_SYSTEM=true
```

## ğŸ“ ìƒˆë¡œìš´ íŒŒì¼ êµ¬ì¡°

```
back-end/
â”œâ”€â”€ redis_client.py          # Redis ì—°ê²° ë° í ì„¤ì •
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_analysis.py    # ì˜¤ë””ì˜¤ ë¶„ì„ ì‘ì—… í•¨ìˆ˜
â”‚   â””â”€â”€ sqs_handler.py       # SQS ë©”ì‹œì§€ ì²˜ë¦¬
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ queue_service.py     # í ê´€ë ¨ ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ analysis_service.py  # ë¶„ì„ ê´€ë ¨ ì„œë¹„ìŠ¤
â””â”€â”€ workers/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ rq_worker.py         # RQ ì›Œì»¤ ì„¤ì •
```

## ğŸ”§ êµ¬í˜„ ì½”ë“œ

### 1. Redis í´ë¼ì´ì–¸íŠ¸ ì„¤ì •

```python
# redis_client.py
import redis
import os
from rq import Queue
from typing import Optional

class RedisClient:
    _instance: Optional[redis.Redis] = None
    
    @classmethod
    def get_client(cls) -> redis.Redis:
        if cls._instance is None:
            cls._instance = redis.Redis(
                host=os.getenv('REDIS_HOST', 'localhost'),
                port=int(os.getenv('REDIS_PORT', 6379)),
                db=int(os.getenv('REDIS_DB', 0)),
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True
            )
        return cls._instance

# í ì¸ìŠ¤í„´ìŠ¤ë“¤
redis_client = RedisClient.get_client()
audio_analysis_queue = Queue('audio_analysis', connection=redis_client)
high_priority_queue = Queue('high_priority', connection=redis_client)
low_priority_queue = Queue('low_priority', connection=redis_client)
```

### 2. ì‘ì—… í•¨ìˆ˜ ì •ì˜

```python
# tasks/audio_analysis.py
import json
import boto3
import logging
from typing import Dict, Any
from database import SessionLocal
from models import AnalysisResult

logger = logging.getLogger(__name__)

def process_audio_analysis(job_id: str, s3_url: str, token_id: int, token_info: Dict[str, Any]) -> Dict[str, Any]:
    """
    RQ ì‘ì—… í•¨ìˆ˜: ì˜¤ë””ì˜¤ ë¶„ì„ ì²˜ë¦¬
    
    Args:
        job_id: ì‘ì—… ID
        s3_url: S3 ì˜¤ë””ì˜¤ íŒŒì¼ URL
        token_id: í† í° ID
        token_info: í† í° ê´€ë ¨ ì •ë³´
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    db = SessionLocal()
    
    try:
        logger.info(f"[ì‘ì—… ì‹œì‘] job_id={job_id}, token_id={token_id}")
        
        # 1. ìƒíƒœ ì—…ë°ì´íŠ¸: ì²˜ë¦¬ ì‹œì‘
        update_analysis_status(db, job_id, "processing", 50, "SQS ë©”ì‹œì§€ ì „ì†¡ ì¤‘...")
        
        # 2. SQS ë©”ì‹œì§€ ìƒì„±
        sqs_message = {
            "job_id": job_id,
            "s3_audio_url": s3_url,
            "token_id": token_id,
            "s3_textgrid_url": token_info.get('s3_textgrid_url'),
            "s3_pitch_url": token_info.get('s3_pitch_url'),
            "callback_url": f"{os.getenv('WEBHOOK_URL')}?job_id={job_id}",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # 3. SQSì— ë©”ì‹œì§€ ì „ì†¡
        message_id = send_to_sqs(sqs_message)
        logger.info(f"[SQS ì „ì†¡ ì™„ë£Œ] job_id={job_id}, message_id={message_id}")
        
        # 4. ìƒíƒœ ì—…ë°ì´íŠ¸: SQS ì „ì†¡ ì™„ë£Œ
        update_analysis_status(
            db, job_id, 
            status="queued_for_analysis", 
            progress=80, 
            message="ë¶„ì„ ì„œë²„ ëŒ€ê¸°ì—´ì— ì¶”ê°€ë¨",
            metadata={"sqs_message_id": message_id}
        )
        
        return {
            "status": "success",
            "job_id": job_id,
            "sqs_message_id": message_id,
            "message": "SQS íì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë¨"
        }
        
    except Exception as e:
        logger.error(f"[ì‘ì—… ì‹¤íŒ¨] job_id={job_id}, error={str(e)}")
        update_analysis_status(db, job_id, "failed", 0, f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise
    finally:
        db.close()

def send_to_sqs(message: Dict[str, Any]) -> str:
    """SQSì— ë©”ì‹œì§€ ì „ì†¡"""
    try:
        sqs = boto3.client(
            'sqs',
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
            region_name=os.getenv('AWS_REGION', 'ap-northeast-2')
        )
        
        queue_url = os.getenv('SQS_QUEUE_URL')
        if not queue_url:
            raise ValueError("SQS_QUEUE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        response = sqs.send_message(
            QueueUrl=queue_url,
            MessageBody=json.dumps(message, ensure_ascii=False),
            MessageAttributes={
                'job_type': {
                    'StringValue': 'audio_analysis',
                    'DataType': 'String'
                },
                'priority': {
                    'StringValue': 'normal',
                    'DataType': 'String'
                }
            }
        )
        
        return response['MessageId']
        
    except Exception as e:
        logger.error(f"SQS ì „ì†¡ ì‹¤íŒ¨: {str(e)}")
        raise

def update_analysis_status(db, job_id: str, status: str, progress: int, message: str, metadata: Dict = None):
    """ë¶„ì„ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    try:
        analysis_result = db.query(AnalysisResult).filter(AnalysisResult.job_id == job_id).first()
        if analysis_result:
            analysis_result.status = status
            analysis_result.progress = progress
            analysis_result.message = message
            if metadata:
                # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©
                existing_metadata = analysis_result.metadata or {}
                existing_metadata.update(metadata)
                analysis_result.metadata = existing_metadata
            db.commit()
            logger.info(f"[ìƒíƒœ ì—…ë°ì´íŠ¸] job_id={job_id}, status={status}, progress={progress}")
    except Exception as e:
        logger.error(f"ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        db.rollback()
```

### 3. í ì„œë¹„ìŠ¤ í´ë˜ìŠ¤

```python
# services/queue_service.py
from typing import Dict, Any, Optional
from rq import Queue
from rq.job import Job
from redis_client import audio_analysis_queue, high_priority_queue, low_priority_queue
import logging

logger = logging.getLogger(__name__)

class QueueService:
    """í ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def enqueue_audio_analysis(
        job_id: str, 
        s3_url: str, 
        token_id: int, 
        token_info: Dict[str, Any],
        priority: str = 'normal'
    ) -> Job:
        """ì˜¤ë””ì˜¤ ë¶„ì„ ì‘ì—…ì„ íì— ì¶”ê°€"""
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ í ì„ íƒ
        queue_map = {
            'high': high_priority_queue,
            'normal': audio_analysis_queue,
            'low': low_priority_queue
        }
        
        selected_queue = queue_map.get(priority, audio_analysis_queue)
        
        job = selected_queue.enqueue(
            'tasks.audio_analysis.process_audio_analysis',
            job_id=job_id,
            s3_url=s3_url,
            token_id=token_id,
            token_info=token_info,
            job_timeout='15m',  # 15ë¶„ íƒ€ì„ì•„ì›ƒ
            retry=3,  # 3íšŒ ì¬ì‹œë„
            job_id=f"audio_analysis_{job_id}"  # RQ job ID ì„¤ì •
        )
        
        logger.info(f"[í ì¶”ê°€] job_id={job_id}, rq_job_id={job.id}, priority={priority}")
        return job
    
    @staticmethod
    def get_job_status(rq_job_id: str) -> Optional[Dict[str, Any]]:
        """RQ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        try:
            job = Job.fetch(rq_job_id, connection=audio_analysis_queue.connection)
            return {
                "id": job.id,
                "status": job.get_status(),
                "created_at": job.created_at,
                "started_at": job.started_at,
                "ended_at": job.ended_at,
                "result": job.result,
                "exc_info": job.exc_info
            }
        except Exception as e:
            logger.error(f"RQ ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None
    
    @staticmethod
    def cancel_job(rq_job_id: str) -> bool:
        """ì‘ì—… ì·¨ì†Œ"""
        try:
            job = Job.fetch(rq_job_id, connection=audio_analysis_queue.connection)
            job.cancel()
            logger.info(f"[ì‘ì—… ì·¨ì†Œ] rq_job_id={rq_job_id}")
            return True
        except Exception as e:
            logger.error(f"ì‘ì—… ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    @staticmethod
    def get_queue_info() -> Dict[str, Any]:
        """í ìƒíƒœ ì •ë³´ ì¡°íšŒ"""
        return {
            "audio_analysis": {
                "length": len(audio_analysis_queue),
                "failed_count": audio_analysis_queue.failed_job_registry.count,
                "started_count": audio_analysis_queue.started_job_registry.count
            },
            "high_priority": {
                "length": len(high_priority_queue),
                "failed_count": high_priority_queue.failed_job_registry.count,
                "started_count": high_priority_queue.started_job_registry.count
            },
            "low_priority": {
                "length": len(low_priority_queue),
                "failed_count": low_priority_queue.failed_job_registry.count,
                "started_count": low_priority_queue.started_job_registry.count
            }
        }
```

### 4. ìˆ˜ì •ëœ API ì—”ë“œí¬ì¸íŠ¸

```python
# router/user_audio_router.py (ìˆ˜ì •ëœ ë¶€ë¶„)
from services.queue_service import QueueService
import os

@router.post("/{token_id}/upload-audio")
async def upload_audio_by_token_id(
    request: Request,
    token_id: str = Path(...),
    file: UploadFile = File(...),
    priority: str = "normal",  # ìš°ì„ ìˆœìœ„ íŒŒë¼ë¯¸í„° ì¶”ê°€
    db: Session = Depends(get_db)
):
    try:
        s3_client = request.app.state.s3_client
        job_id = str(uuid4())
        file_data = await file.read()
        token_info = await get_token_by_id(token_id, db)
        
        # DBì— ì´ˆê¸° ìƒíƒœ ì €ì¥
        analysis_result = create_analysis_result(
            db, job_id, int(token_id), 
            status="uploading", 
            progress=10, 
            message="S3 ì—…ë¡œë“œ ì¤‘..."
        )
        
        # S3 ì—…ë¡œë“œ
        update_analysis_result(db, job_id, progress=30, message="S3 ì—…ë¡œë“œ ì¤‘...")
        s3_key = await upload_to_s3_async(s3_client, file_data, file.filename)
        s3_url = f"s3://{S3_BUCKET}/{s3_key}"
        
        # í ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        if os.getenv('USE_QUEUE_SYSTEM', 'false').lower() == 'true':
            # ìƒˆë¡œìš´ í ë°©ì‹
            update_analysis_result(db, job_id, status="queuing", progress=40, message="ì‘ì—… íì— ì¶”ê°€ ì¤‘...")
            
            # RQ íì— ì‘ì—… ì¶”ê°€
            rq_job = QueueService.enqueue_audio_analysis(
                job_id=job_id,
                s3_url=s3_url,
                token_id=int(token_id),
                token_info={
                    'id': token_info.id,
                    's3_textgrid_url': getattr(token_info, 's3_textgrid_url', None),
                    's3_pitch_url': getattr(token_info, 's3_pitch_url', None)
                },
                priority=priority
            )
            
            update_analysis_result(
                db, job_id, 
                status="queued", 
                progress=50, 
                message="ì‘ì—… íì— ì¶”ê°€ë¨",
                metadata={"rq_job_id": rq_job.id}
            )
            
            return {
                "message": "ì—…ë¡œë“œ ì™„ë£Œ, ì‘ì—… íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "job_id": job_id,
                "rq_job_id": rq_job.id,
                "status": "queued",
                "priority": priority,
                "queue_position": len(audio_analysis_queue) if priority == 'normal' else 0
            }
        else:
            # ê¸°ì¡´ httpx ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
            return await upload_with_httpx_legacy(...)
            
    except Exception as e:
        logging.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        if 'analysis_result' in locals():
            update_analysis_result(db, job_id, status="failed", progress=0, message=f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì—ëŸ¬: {str(e)}")

# í ìƒíƒœ ì¡°íšŒ API ì¶”ê°€
@router.get("/queue/status")
def get_queue_status():
    """í ìƒíƒœ ì¡°íšŒ"""
    return QueueService.get_queue_info()

# ì‘ì—… ì·¨ì†Œ API ì¶”ê°€
@router.delete("/jobs/{job_id}/cancel")
def cancel_analysis_job(job_id: str, db: Session = Depends(get_db)):
    """ë¶„ì„ ì‘ì—… ì·¨ì†Œ"""
    analysis_result = get_analysis_result(db, job_id)
    if not analysis_result:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if analysis_result.status in ["completed", "failed"]:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # RQ ì‘ì—… ì·¨ì†Œ
    metadata = analysis_result.metadata or {}
    rq_job_id = metadata.get('rq_job_id')
    
    if rq_job_id:
        success = QueueService.cancel_job(rq_job_id)
        if success:
            update_analysis_result(db, job_id, status="cancelled", message="ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨")
            return {"message": "ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", "job_id": job_id}
    
    raise HTTPException(status_code=500, detail="ì‘ì—… ì·¨ì†Œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
```

### 5. ì›Œì»¤ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# workers/rq_worker.py
import os
import sys
import logging
from rq import Worker
from redis_client import redis_client

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_worker():
    """RQ ì›Œì»¤ ì‹¤í–‰"""
    # í ì´ë¦„ë“¤
    queue_names = ['high_priority', 'audio_analysis', 'low_priority']
    
    # ì›Œì»¤ ìƒì„±
    worker = Worker(
        queue_names,
        connection=redis_client,
        name=f"worker-{os.getpid()}",
        default_result_ttl=3600  # ê²°ê³¼ 1ì‹œê°„ ë³´ê´€
    )
    
    logger.info(f"RQ Worker ì‹œì‘: PID={os.getpid()}, Queues={queue_names}")
    
    try:
        worker.work(with_scheduler=True)
    except KeyboardInterrupt:
        logger.info("ì›Œì»¤ ì¢…ë£Œ ì¤‘...")
        worker.stop()

if __name__ == '__main__':
    run_worker()
```

### 6. ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# 1. FastAPI ì„œë²„ ì‹¤í–‰
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 2. RQ ì›Œì»¤ ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
python workers/rq_worker.py

# 3. RQ ëŒ€ì‹œë³´ë“œ ì‹¤í–‰ (ì„ íƒì‚¬í•­, ë³„ë„ í„°ë¯¸ë„)
rq-dashboard --redis-url redis://localhost:6379

# 4. ì—¬ëŸ¬ ì›Œì»¤ ì‹¤í–‰ (ì„±ëŠ¥ í–¥ìƒ)
python workers/rq_worker.py &
python workers/rq_worker.py &
python workers/rq_worker.py &
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### RQ ëŒ€ì‹œë³´ë“œ ì ‘ì†
- URL: http://localhost:9181
- ì‹¤ì‹œê°„ í ìƒíƒœ, ì‘ì—… ì§„í–‰ ìƒí™©, ì‹¤íŒ¨í•œ ì‘ì—… í™•ì¸ ê°€ëŠ¥

### ë¡œê·¸ í™•ì¸
```bash
# FastAPI ë¡œê·¸
tail -f server.log

# RQ ì›Œì»¤ ë¡œê·¸
tail -f worker.log
```

### Redis ìƒíƒœ í™•ì¸
```bash
redis-cli info
redis-cli monitor  # ì‹¤ì‹œê°„ ëª…ë ¹ì–´ ëª¨ë‹ˆí„°ë§
```

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Redis ì—°ê²° ì‹¤íŒ¨
```bash
# Redis ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
brew services list | grep redis

# Redis ì¬ì‹œì‘
brew services restart redis
```

### 2. ì‘ì—…ì´ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ
- ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
- íì— ì‘ì—…ì´ ìŒ“ì—¬ìˆëŠ”ì§€ RQ ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸
- Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

### 3. SQS ê¶Œí•œ ì˜¤ë¥˜
```bash
# AWS ìê²© ì¦ëª… í™•ì¸
aws sts get-caller-identity

# SQS í ê¶Œí•œ í™•ì¸
aws sqs get-queue-attributes --queue-url YOUR_QUEUE_URL --attribute-names All
```

ì´ì œ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•˜ì‹œë©´ ë©ë‹ˆë‹¤! ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.
