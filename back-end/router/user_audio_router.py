# router/user_audio_router.py

import os, logging, boto3, asyncio, json
from uuid import uuid4
from typing import List
from fastapi import APIRouter, UploadFile, File, Path, HTTPException, Request, BackgroundTasks, Depends, Form
from fastapi.responses import StreamingResponse
from concurrent.futures import ThreadPoolExecutor
import httpx
from database import get_db
from sqlalchemy.orm import Session
from models import Token, AnalysisResult, User
from services.sqs_service import sqs_service
from router.auth_router import get_current_user  # ì¸ì¦ í•¨ìˆ˜ import


async def get_token_by_id(token_id: str, db:Session):
    token = db.query(Token).filter(Token.id == int(token_id)).first()
    if not token:
        raise HTTPException(status_code=404, detail="Token not found")
    return token

# DB í—¬í¼ í•¨ìˆ˜ë“¤
def create_analysis_result(db: Session, job_id: str, token_id: int, user_id: int = None, status: str = "processing", progress: int = 10, message: str = "ì—…ë¡œë“œ ì‹œìž‘"):
    analysis_result = AnalysisResult(
        job_id=job_id,
        token_id=token_id,
        user_id=user_id,  # ì‚¬ìš©ìž ID ì¶”ê°€
        status=status,
        progress=progress,
        message=message
    )
    db.add(analysis_result)
    db.commit()
    db.refresh(analysis_result)
    return analysis_result

def update_analysis_result(db: Session, job_id: str, **kwargs):
    analysis_result = db.query(AnalysisResult).filter(AnalysisResult.job_id == job_id).first()
    if analysis_result:
        for key, value in kwargs.items():
            setattr(analysis_result, key, value)
        db.commit()
        db.refresh(analysis_result)
    return analysis_result

def get_analysis_result(db: Session, job_id: str):
    return db.query(AnalysisResult).filter(AnalysisResult.job_id == job_id).first()



# ===============================================

router = APIRouter(
    prefix="/tokens",
    tags=["tokens"]
)

S3_BUCKET = os.getenv("S3_BUCKET_NAME")
TARGET_URL = os.getenv("TARGET_SERVER_URL")
WEBHOOK_URL = os.getenv("WEBHOOK_URL")

# ë¹„ë™ê¸° S3 ì—…ë¡œë“œ í•¨ìˆ˜
async def upload_to_s3_async(s3_client, file_data: bytes, filename: str) -> str:
    """ë¹„ë™ê¸° S3 ì—…ë¡œë“œ"""
    def sync_upload():
        import io
        key = f"audio/{uuid4().hex}_{filename}"
        s3_client.upload_fileobj(io.BytesIO(file_data), S3_BUCKET, key)
        return key
    
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as executor:
        return await loop.run_in_executor(executor, sync_upload)

# SQS ë©”ì‹œì§€ ì „ì†¡ í•¨ìˆ˜
async def send_to_sqs_async(s3_url: str, token_id: str, webhook_url: str, job_id: str, token_info: Token):
    """SQSë¥¼ ì‚¬ìš©í•œ ë¶„ì„ ìš”ì²­"""
    try:
        # í† í° ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        token_info_dict = {
            's3_textgrid_url': getattr(token_info, 's3_textgrid_url', None),
            's3_pitch_url': getattr(token_info, 's3_pitch_url', None)
        }
        
        # SQSì— ë©”ì‹œì§€ ì „ì†¡
        message_id = sqs_service.send_analysis_message(
            job_id=job_id,
            s3_audio_url=s3_url,
            token_id=str(token_id),
            webhook_url=webhook_url,
            token_info=token_info_dict
        )
        
        if message_id:
            logging.info(f"[SQS ì „ì†¡ ì„±ê³µ] job_id={job_id}, message_id={message_id}")
            return {"message_id": message_id, "status": "queued"}
        else:
            raise Exception("SQS ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")
            
    except Exception as e:
        logging.error(f"[SQS ì „ì†¡ ì‹¤íŒ¨] job_id={job_id}, error={str(e)}")
        raise

# ë¹„ë™ê¸° HTTP ìš”ì²­ í•¨ìˆ˜ (ê¸°ì¡´ ë°©ì‹ - í•˜ìœ„ í˜¸í™˜ìš©)
async def send_analysis_request_async(s3_url: str, token_id: str, webhook_url: str, job_id: str, token_info: Token):
    """httpxë¥¼ ì‚¬ìš©í•œ ì™„ì „ ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­"""
    try:
        async with httpx.AsyncClient(timeout=70.0) as client:
            response = await client.post(
                TARGET_URL,
                data={
                    "s3_audio_url": s3_url,
                    "video_id": token_id,
                    "webhook_url": webhook_url,
                    "s3_textgrid_url": f"s3://testgrid-pitch-bgvoice-yousync/{token_info.s3_textgrid_url}" if token_info.s3_textgrid_url else None,
                    "s3_pitch_url": f"s3://testgrid-pitch-bgvoice-yousync/{token_info.s3_pitch_url}" if token_info.s3_pitch_url else None
                },
                headers={"Content-Type": "application/x-www-form-urlencoded"}
            )
            response.raise_for_status()
            logging.info(f"[ë¶„ì„ ìš”ì²­ ì„±ê³µ] job_id={job_id}")
            
            # ì‘ë‹µ ë°ì´í„° í™•ì¸ ë° ë°˜í™˜
            if response.text and response.text.strip():
                try:
                    response_data = response.json()
                    logging.info(f"[ë¶„ì„ ì„œë²„ ì‘ë‹µ] {len(str(response_data))} ë¬¸ìž")
                    return response_data
                except:
                    logging.info(f"[ë¶„ì„ ì„œë²„ í…ìŠ¤íŠ¸ ì‘ë‹µ] {response.text}")
                    
    except httpx.HTTPStatusError as e:
        logging.error(f"[ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨] job_id={job_id}, status={e.response.status_code}, body={e.response.text}")
        raise
    except Exception as e:
        logging.error(f"[ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨] job_id={job_id}, error={e}")
        raise

# 1. ì˜¤ë””ì˜¤ ì—…ë¡œë“œ ë° ë¶„ì„ ìš”ì²­ (ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬)
@router.post("/{token_id}/upload-audio/")
async def upload_audio_by_token_id(
    request: Request,
    token_id: str = Path(...),
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)  # ðŸ” ë¡œê·¸ì¸í•œ ì‚¬ìš©ìžë§Œ í˜¸ì¶œ ê°€ëŠ¥
    
):
    try:
        s3_client = request.app.state.s3_client
        job_id = str(uuid4())
        file_data = await file.read()
        token_info = await get_token_by_id(token_id, db)
        
        # DBì— ì´ˆê¸° ìƒíƒœ ì €ìž¥
        analysis_result = create_analysis_result(db, job_id, int(token_id), current_user.id)

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬
        async def process_in_background(s3_client_bg):
            # ìƒˆë¡œìš´ DB ì„¸ì…˜ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ ìž‘ì—…ìš©)
            from database import SessionLocal
            bg_db = SessionLocal()
            try:
                # S3 ì—…ë¡œë“œ
                update_analysis_result(bg_db, job_id, progress=40, message="S3 ì—…ë¡œë“œ ì¤‘...")
                
                s3_key = await upload_to_s3_async(s3_client_bg, file_data, file.filename)
                s3_url = f"s3://{S3_BUCKET}/{s3_key}"
                
                webhook_url = f"{WEBHOOK_URL}?job_id={job_id}"
                
                # í™˜ê²½ ë³€ìˆ˜ë¡œ SQS ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                use_sqs = os.getenv('USE_SQS_QUEUE', 'false').lower() == 'true'
                
                if use_sqs:
                    # SQS ë°©ì‹
                    update_analysis_result(bg_db, job_id, progress=70, message="SQS íì— ë©”ì‹œì§€ ì „ì†¡ ì¤‘...")
                    
                    sqs_result = await send_to_sqs_async(
                        s3_url, 
                        token_info.id, 
                        webhook_url, 
                        job_id,
                        token_info
                    )
                    
                    update_analysis_result(
                        bg_db, job_id, 
                        status="queued_for_analysis", 
                        progress=90, 
                        message="SQS íì— ì „ì†¡ ì™„ë£Œ, ë¶„ì„ ëŒ€ê¸° ì¤‘...",
                        metadata={"sqs_message_id": sqs_result.get("message_id")}
                    )
                    logging.info(f"[SQS ë°©ì‹ ì™„ë£Œ] job_id={job_id}")
                    
                else:
                    # ê¸°ì¡´ HTTP ë°©ì‹
                    update_analysis_result(bg_db, job_id, progress=70, message="ë¶„ì„ ì„œë²„ ìš”ì²­ ì¤‘...")
                    
                    response_data = await send_analysis_request_async(
                        s3_url, 
                        token_info.id, 
                        webhook_url, 
                        job_id,
                        token_info
                    )
                    
                    # POST ì‘ë‹µì—ì„œ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ê°€ ìžˆëŠ”ì§€ í™•ì¸
                    if response_data and isinstance(response_data, dict) and 'scores' in response_data:
                        # ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì€ ê²½ìš°
                        update_analysis_result(bg_db, job_id, 
                                              status="completed", 
                                              progress=100, 
                                              result=response_data, 
                                              message="ë¶„ì„ ì™„ë£Œ")
                        logging.info(f"[HTTP POST ì‘ë‹µìœ¼ë¡œ ë¶„ì„ ì™„ë£Œ] job_id={job_id}")
                    else:
                        # ì›¹í›… ëŒ€ê¸° ìƒíƒœë¡œ ì„¤ì •
                        update_analysis_result(bg_db, job_id, progress=90, message="ë¶„ì„ ì¤‘... ê²°ê³¼ ëŒ€ê¸°")
                        logging.info(f"[HTTP ì›¹í›… ëŒ€ê¸°] job_id={job_id}")
                
            except Exception as e:
                    # ì›¹í›… ëŒ€ê¸° ìƒíƒœë¡œ ì„¤ì •
                    update_analysis_result(bg_db, job_id, progress=90, message="ë¶„ì„ ì¤‘... ê²°ê³¼ ëŒ€ê¸°")
                    logging.info(f"[ì›¹í›… ëŒ€ê¸°] job_id={job_id}")
                
            except Exception as e:
                logging.error(f"ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                update_analysis_result(bg_db, job_id, status="failed", message=str(e), progress=0)
            finally:
                bg_db.close()

        background_tasks.add_task(process_in_background, s3_client)
        
        return {
            "message": "ì—…ë¡œë“œ ì™„ë£Œ, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.",
            "job_id": job_id,
            "status": "processing",
            "token_info": {
                "id": token_info.id,
                "s3_textgrid_url": getattr(token_info, 's3_textgrid_url', None),
                "s3_pitch_url": getattr(token_info, 's3_pitch_url', None)
            }
        }
        
    except Exception as e:
        logging.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì—ëŸ¬: {str(e)}")


# 2. ë¶„ì„ ê²°ê³¼ë¥¼ ìˆ˜ì‹ í•  ì›¹í›… ì—”ë“œí¬ì¸íŠ¸
@router.post("/webhook/analysis-complete/")
async def receive_analysis(request: Request, db: Session = Depends(get_db)):
    from fastapi.responses import JSONResponse
    
    # ì›¹í›… í˜¸ì¶œ ë¡œê¹… ì¶”ê°€
    logging.info("=" * 50)
    logging.info("[ðŸ”” ì›¹í›… í˜¸ì¶œë¨] Token ë¶„ì„ ê²°ê³¼ ì›¹í›… ìˆ˜ì‹ ")
    logging.info(f"[ì›¹í›… ìš”ì²­ IP] {request.client.host if request.client else 'Unknown'}")
    logging.info(f"[ì›¹í›… í—¤ë”] {dict(request.headers)}")
    
    job_id = request.query_params.get("job_id")
    task_id = request.query_params.get("task_id")
    
    logging.info(f"[ì›¹í›… íŒŒë¼ë¯¸í„°] job_id={job_id}, task_id={task_id}")

    if not job_id:
        logging.warning("[â—ê²½ê³ ] job_id ì—†ì´ webhook ë„ì°©. ë¬´ì‹œë¨")
        return JSONResponse(status_code=400, content={"error": "job_id is required"})

    # ì´ë¯¸ ì™„ë£Œëœ job_idì¸ì§€ í™•ì¸
    existing_result = get_analysis_result(db, job_id)
    if existing_result and existing_result.status == "completed":
        logging.info(f"[ì¤‘ë³µ ìš”ì²­ ë¬´ì‹œ] job_id={job_id}ëŠ” ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ")
        return {"received": True, "job_id": job_id, "message": "ì´ë¯¸ ì™„ë£Œëœ ìž‘ì—…"}

    data = await request.json()
    # analysis_results í‚¤ê°€ ìžˆìœ¼ë©´ ê·¸ê²ƒì„, ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©
    if "analysis_results" in data:
        results = data.get("analysis_results", {})
    else:
        results = data  # ì „ì²´ ë°ì´í„°ë¥¼ ê²°ê³¼ë¡œ ì‚¬ìš©
    
    logging.info(f"[ì›¹í›… ë°ì´í„°] ë°›ì€ ê²°ê³¼ í¬ê¸°: {len(str(results))} ë¬¸ìž")
    logging.info(f"[ì›¹í›… ë°ì´í„°] ê²°ê³¼ í‚¤ë“¤: {list(results.keys()) if isinstance(results, dict) else 'Not dict'}")
    
    # ë¶„ì„ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
    update_analysis_result(db, job_id, 
                          status="completed", 
                          progress=100, 
                          result=results, 
                          message="ë¶„ì„ ì™„ë£Œ")

    logging.info(f"[âœ… ì›¹í›… ì²˜ë¦¬ ì™„ë£Œ] job_id={job_id}, task_id={task_id}")
    logging.info("=" * 50)
    return {"received": True, "job_id": job_id, "task_id": task_id}


# 3. í´ë¼ì´ì–¸íŠ¸ê°€ ì¡°íšŒí•  ìˆ˜ ìžˆëŠ” ê²°ê³¼ ì¡°íšŒ API
@router.get("/analysis-result/{job_id}/")
def get_analysis_result_api(job_id: str, db: Session = Depends(get_db)):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    stored_data = get_analysis_result(db, job_id)
    if not stored_data:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {
        "job_id": stored_data.job_id,
        "token_id": stored_data.token_id,
        "status": stored_data.status,
        "progress": stored_data.progress,
        "result": stored_data.result,
        "message": stored_data.message,
        "created_at": stored_data.created_at
    }


# 4. ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ ìœ„í•œ Server-Sent Events ì—”ë“œí¬ì¸íŠ¸
@router.get("/analysis-progress/{job_id}/")
async def stream_analysis_progress(job_id: str):
    """ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë° (SSE)"""
    async def event_generator():
        from database import SessionLocal
        
        while True:
            try:
                # ìƒˆë¡œìš´ DB ì„¸ì…˜ìœ¼ë¡œ í˜„ìž¬ ìƒíƒœ ì¡°íšŒ
                db = SessionLocal()
                try:
                    current_data = get_analysis_result(db, job_id)
                    if not current_data:
                        yield f"data: {json.dumps({'error': 'Job not found'}, ensure_ascii=False)}\n\n"
                        break
                    
                    data_dict = {
                        "job_id": current_data.job_id,
                        "status": current_data.status,
                        "progress": current_data.progress,
                        "message": current_data.message,
                        "result": current_data.result
                    }
                    
                    # ì™„ë£Œëœ ê²½ìš° ë§ˆì§€ë§‰ ë°ì´í„° ì „ì†¡ í›„ ì¢…ë£Œ
                    if current_data.status in ["completed", "failed"]:
                        yield f"data: {json.dumps(data_dict, ensure_ascii=False)}\n\n"
                        break
                    
                    yield f"data: {json.dumps(data_dict, ensure_ascii=False)}\n\n"
                finally:
                    db.close()
                    
                await asyncio.sleep(3)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                
            except Exception as e:
                logging.error(f"SSE ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
                error_data = {
                    "status": "error", 
                    "error": str(e),
                    "job_id": job_id
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                break
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

# SQS í ìƒíƒœ ì¡°íšŒ API
@router.get("/queue/status/")
def get_sqs_queue_status():
    """SQS í ìƒíƒœ ì¡°íšŒ"""
    try:
        queue_attributes = sqs_service.get_queue_attributes()
        
        if queue_attributes:
            return {
                "status": "success",
                "queue_info": {
                    "messages_available": queue_attributes.get('ApproximateNumberOfMessages', '0'),
                    "messages_in_flight": queue_attributes.get('ApproximateNumberOfMessagesNotVisible', '0'),
                    "queue_url": os.getenv('SQS_QUEUE_URL', 'Not configured')
                },
                "sqs_enabled": os.getenv('USE_SQS_QUEUE', 'false').lower() == 'true'
            }
        else:
            return {
                "status": "error",
                "message": "SQS í ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sqs_enabled": os.getenv('USE_SQS_QUEUE', 'false').lower() == 'true'
            }
    except Exception as e:
        return {
            "status": "error",
            "message": f"SQS ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}",
            "sqs_enabled": os.getenv('USE_SQS_QUEUE', 'false').lower() == 'true'
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸš€ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ (ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/batch-upload/")
async def batch_upload_audio(
    request: Request,
    files: List[UploadFile] = File(...),
    token_ids: str = Form(...),  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìžì—´ë¡œ ë°›ê¸°
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•˜ê³  ë³‘ë ¬ë¡œ ë¶„ì„ ìš”ì²­
    
    Args:
        files: ì—…ë¡œë“œí•  ì˜¤ë””ì˜¤ íŒŒì¼ë“¤
        token_ids: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ token_id ë¬¸ìžì—´ (ì˜ˆ: "1,2,3")
    
    Returns:
        job_ids: ê° íŒŒì¼ì˜ ìž‘ì—… ID ë¦¬ìŠ¤íŠ¸
    """
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìžì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    token_id_list = [tid.strip() for tid in token_ids.split(",") if tid.strip()]
    
    if len(files) != len(token_id_list):
        raise HTTPException(400, f"íŒŒì¼ ìˆ˜({len(files)})ì™€ token_id ìˆ˜({len(token_id_list)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    s3_client = request.app.state.s3_client
    job_ids = []
    
    # ê° íŒŒì¼ì— ëŒ€í•´ job_id ìƒì„± ë° ì´ˆê¸° ìƒíƒœ ì €ìž¥
    for i, (file, token_id) in enumerate(zip(files, token_id_list)):
        job_id = str(uuid4())
        job_ids.append(job_id)
        
        # DBì— ì´ˆê¸° ìƒíƒœ ì €ìž¥
        create_analysis_result(db, job_id, int(token_id), current_user.id)
        logging.info(f"[ë°°ì¹˜ ìž‘ì—… ìƒì„±] job_id={job_id}, file={file.filename}, token_id={token_id}")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìž‘
    background_tasks.add_task(
        process_batch_files_parallel,
        s3_client,
        [(await file.read(), file.filename) for file in files],  # íŒŒì¼ ë°ì´í„° ë¯¸ë¦¬ ì½ê¸°
        token_id_list,
        job_ids,
        current_user.id
    )
    
    return {
        "message": f"{len(files)}ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìž‘",
        "job_ids": job_ids,
        "total_files": len(files)
    }

async def process_batch_files_parallel(
    s3_client,
    file_data_list: List[tuple],  # (file_data, filename) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    token_ids: List[str],
    job_ids: List[str],
    user_id: int
):
    """ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ í•¨ìˆ˜"""
    
    async def process_single_file_async(file_data: bytes, filename: str, token_id: str, job_id: str):
        """ë‹¨ì¼ íŒŒì¼ ë¹„ë™ê¸° ì²˜ë¦¬"""
        from database import SessionLocal
        bg_db = SessionLocal()
        
        try:
            logging.info(f"[ë°°ì¹˜ ì²˜ë¦¬ ì‹œìž‘] job_id={job_id}, file={filename}")
            
            # í† í° ì •ë³´ ì¡°íšŒ
            token_info = await get_token_by_id(token_id, bg_db)
            
            # 1. S3 ì—…ë¡œë“œ
            update_analysis_result(bg_db, job_id, progress=20, message="S3 ì—…ë¡œë“œ ì¤‘...")
            s3_key = await upload_to_s3_async(s3_client, file_data, filename)
            s3_url = f"s3://{S3_BUCKET}/{s3_key}"
            
            # 2. ë¶„ì„ ì„œë²„ ìš”ì²­ ì¤€ë¹„
            update_analysis_result(bg_db, job_id, progress=50, message="ë¶„ì„ ì„œë²„ ìš”ì²­ ì¤‘...")
            webhook_url = f"{WEBHOOK_URL}?job_id={job_id}"
            
            # 3. ë¶„ì„ ì„œë²„ì— ë¹„ë™ê¸° ìš”ì²­
            await send_analysis_request_async(
                s3_url, token_id, webhook_url, job_id, token_info
            )
            
            # 4. ìš”ì²­ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            update_analysis_result(
                bg_db, job_id, 
                status="processing", 
                progress=80, 
                message="ë¶„ì„ ì„œë²„ì—ì„œ ì²˜ë¦¬ ì¤‘..."
            )
            
            logging.info(f"[ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ] job_id={job_id}, file={filename}")
            
        except Exception as e:
            logging.error(f"[ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨] job_id={job_id}, file={filename}, error={e}")
            update_analysis_result(
                bg_db, job_id, 
                status="failed", 
                progress=0, 
                message=f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            )
        finally:
            bg_db.close()
    
    # ðŸš€ í•µì‹¬: asyncio.gatherë¡œ ëª¨ë“  íŒŒì¼ì„ ë™ì‹œì— ì²˜ë¦¬
    tasks = [
        process_single_file_async(file_data, filename, token_id, job_id)
        for (file_data, filename), token_id, job_id in zip(file_data_list, token_ids, job_ids)
    ]
    
    try:
        # ëª¨ë“  ìž‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        await asyncio.gather(*tasks, return_exceptions=True)
        logging.info(f"[ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ] ì´ {len(file_data_list)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logging.error(f"[ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜] {e}")

@router.get("/batch-progress/")
async def get_batch_progress(
    job_ids: str,  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ job_id ë¬¸ìžì—´
    db: Session = Depends(get_db)
):
    """
    ë°°ì¹˜ ìž‘ì—…ë“¤ì˜ ì§„í–‰ ìƒí™© ì¡°íšŒ
    
    Args:
        job_ids: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ job_id ë¬¸ìžì—´ (ì˜ˆ: "job1,job2,job3")
    """
    job_id_list = [jid.strip() for jid in job_ids.split(",") if jid.strip()]
    
    if not job_id_list:
        raise HTTPException(400, "job_idsê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤")
    
    results = []
    completed_count = 0
    failed_count = 0
    
    for job_id in job_id_list:
        result = get_analysis_result(db, job_id)
        if result:
            status_info = {
                "job_id": result.job_id,
                "status": result.status,
                "progress": result.progress,
                "message": result.message,
                "token_id": result.token_id
            }
            
            if result.status == "completed":
                completed_count += 1
            elif result.status == "failed":
                failed_count += 1
                
            results.append(status_info)
        else:
            results.append({
                "job_id": job_id,
                "status": "not_found",
                "progress": 0,
                "message": "ìž‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            })
            failed_count += 1
    
    # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚°
    total_jobs = len(job_id_list)
    overall_progress = (completed_count / total_jobs) * 100 if total_jobs > 0 else 0
    
    return {
        "batch_results": results,
        "summary": {
            "total_jobs": total_jobs,
            "completed": completed_count,
            "failed": failed_count,
            "in_progress": total_jobs - completed_count - failed_count,
            "overall_progress": round(overall_progress, 1)
        }
    }