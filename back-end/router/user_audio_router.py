# router/user_audio_router.py

import os, logging, boto3, asyncio, json
from uuid import uuid4
from fastapi import APIRouter, UploadFile, File, Path, HTTPException, Request, BackgroundTasks, Depends
from fastapi.responses import StreamingResponse
from concurrent.futures import ThreadPoolExecutor
import httpx
from database import get_db
from sqlalchemy.orm import Session
from models import Token, AnalysisResult, User


async def get_token_by_id(token_id: str, db:Session):
    token = db.query(Token).filter(Token.id == int(token_id)).first()
    if not token:
        raise HTTPException(status_code=404, detail="Token not found")
    return token

# DB í—¬í¼ í•¨ìˆ˜ë“¤
def create_analysis_result(db: Session, job_id: str, token_id: int, status: str = "processing", progress: int = 10, message: str = "ì—…ë¡œë“œ ì‹œì‘"):
    analysis_result = AnalysisResult(
        job_id=job_id,
        token_id=token_id,
        status=status,
        progress=progress,
        message=message
    )
    db.add(analysis_result)
    db.commit()
    db.refresh(analysis_result)
    return analysis_result

def update_analysis_result(db: Session, job_id: str, **kwargs):
    analysis_result = db.query(AnalysisResult).filter(AnalysisResult.job_id == job_id).first()
    if analysis_result:
        for key, value in kwargs.items():
            setattr(analysis_result, key, value)
        db.commit()
        db.refresh(analysis_result)
    return analysis_result

def get_analysis_result(db: Session, job_id: str):
    return db.query(AnalysisResult).filter(AnalysisResult.job_id == job_id).first()



# ===============================================

router = APIRouter(
    prefix="/tokens",
    tags=["tokens"]
)

S3_BUCKET = os.getenv("S3_BUCKET_NAME")
TARGET_URL = os.getenv("TARGET_SERVER_URL")
WEBHOOK_URL = os.getenv("WEBHOOK_URL")

# ë¹„ë™ê¸° S3 ì—…ë¡œë“œ í•¨ìˆ˜
async def upload_to_s3_async(s3_client, file_data: bytes, filename: str) -> str:
    """ë¹„ë™ê¸° S3 ì—…ë¡œë“œ"""
    def sync_upload():
        import io
        key = f"audio/{uuid4().hex}_{filename}"
        s3_client.upload_fileobj(io.BytesIO(file_data), S3_BUCKET, key)
        return key
    
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as executor:
        return await loop.run_in_executor(executor, sync_upload)

# ë¹„ë™ê¸° HTTP ìš”ì²­ í•¨ìˆ˜
async def send_analysis_request_async(s3_url: str, token_id: str, webhook_url: str, job_id: str, token_info: Token):
    """httpxë¥¼ ì‚¬ìš©í•œ ì™„ì „ ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­"""
    try:
        async with httpx.AsyncClient(timeout=70.0) as client:
            response = await client.post(
                TARGET_URL,
                data={
                    "s3_audio_url": s3_url,
                    "video_id": token_id,
                    "webhook_url": webhook_url,
                    "s3_textgrid_url": f"s3://testgrid-pitch-bgvoice-yousync/{token_info.s3_textgrid_url}" if token_info.s3_textgrid_url else None,
                    "s3_pitch_url": f"s3://testgrid-pitch-bgvoice-yousync/{token_info.s3_pitch_url}" if token_info.s3_pitch_url else None
                },
                headers={"Content-Type": "application/x-www-form-urlencoded"}
            )
            response.raise_for_status()
            logging.info(f"[ë¶„ì„ ìš”ì²­ ì„±ê³µ] job_id={job_id}")
            
            # ì‘ë‹µ ë°ì´í„° í™•ì¸ ë° ë°˜í™˜
            if response.text and response.text.strip():
                try:
                    response_data = response.json()
                    logging.info(f"[ë¶„ì„ ì„œë²„ ì‘ë‹µ] {len(str(response_data))} ë¬¸ì")
                    return response_data
                except:
                    logging.info(f"[ë¶„ì„ ì„œë²„ í…ìŠ¤íŠ¸ ì‘ë‹µ] {response.text}")
                    
    except httpx.HTTPStatusError as e:
        logging.error(f"[ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨] job_id={job_id}, status={e.response.status_code}, body={e.response.text}")
        raise
    except Exception as e:
        logging.error(f"[ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨] job_id={job_id}, error={e}")
        raise

# 1. ì˜¤ë””ì˜¤ ì—…ë¡œë“œ ë° ë¶„ì„ ìš”ì²­ (ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬)
@router.post("/{token_id}/upload-audio")
async def upload_audio_by_token_id(
    request: Request,
    token_id: str = Path(...),
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: Session = Depends(get_db),
    #current_user: User = Depends(get_current_user)  # ğŸ” ë¡œê·¸ì¸í•œ ì‚¬ìš©ìë§Œ í˜¸ì¶œ ê°€ëŠ¥
    
):
    try:
        s3_client = request.app.state.s3_client
        job_id = str(uuid4())
        file_data = await file.read()
        token_info = await get_token_by_id(token_id, db)
        
        # DBì— ì´ˆê¸° ìƒíƒœ ì €ì¥
        analysis_result = create_analysis_result(db, job_id, int(token_id))

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬
        async def process_in_background(s3_client_bg):
            # ìƒˆë¡œìš´ DB ì„¸ì…˜ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìš©)
            from database import SessionLocal
            bg_db = SessionLocal()
            try:
                # S3 ì—…ë¡œë“œ
                update_analysis_result(bg_db, job_id, progress=40, message="S3 ì—…ë¡œë“œ ì¤‘...")
                
                s3_key = await upload_to_s3_async(s3_client_bg, file_data, file.filename)
                s3_url = f"s3://{S3_BUCKET}/{s3_key}"
                
                update_analysis_result(bg_db, job_id, progress=70, message="ë¶„ì„ ì„œë²„ ìš”ì²­ ì¤‘...")
                
                # ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­
                webhook_url = f"{WEBHOOK_URL}?job_id={job_id}"
                response_data = await send_analysis_request_async(
                    s3_url, 
                    token_info.id, 
                    webhook_url, 
                    job_id,
                    token_info
                )
                
                # POST ì‘ë‹µì—ì„œ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if response_data and isinstance(response_data, dict) and 'scores' in response_data:
                    # ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì€ ê²½ìš°
                    update_analysis_result(bg_db, job_id, 
                                          status="completed", 
                                          progress=100, 
                                          result=response_data, 
                                          message="ë¶„ì„ ì™„ë£Œ")
                    logging.info(f"[POST ì‘ë‹µìœ¼ë¡œ ë¶„ì„ ì™„ë£Œ] job_id={job_id}")
                else:
                    # ì›¹í›… ëŒ€ê¸° ìƒíƒœë¡œ ì„¤ì •
                    update_analysis_result(bg_db, job_id, progress=90, message="ë¶„ì„ ì¤‘... ê²°ê³¼ ëŒ€ê¸°")
                    logging.info(f"[ì›¹í›… ëŒ€ê¸°] job_id={job_id}")
                
            except Exception as e:
                logging.error(f"ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                update_analysis_result(bg_db, job_id, status="failed", message=str(e), progress=0)
            finally:
                bg_db.close()

        background_tasks.add_task(process_in_background, s3_client)
        
        return {
            "message": "ì—…ë¡œë“œ ì™„ë£Œ, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.",
            "job_id": job_id,
            "status": "processing",
            "token_info": {
                "id": token_info.id,
                "s3_textgrid_url": getattr(token_info, 's3_textgrid_url', None),
                "s3_pitch_url": getattr(token_info, 's3_pitch_url', None)
            }
        }
        
    except Exception as e:
        logging.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì—ëŸ¬: {str(e)}")


# 2. ë¶„ì„ ê²°ê³¼ë¥¼ ìˆ˜ì‹ í•  ì›¹í›… ì—”ë“œí¬ì¸íŠ¸
@router.post("/webhook/analysis-complete")
async def receive_analysis(request: Request, db: Session = Depends(get_db)):
    from fastapi.responses import JSONResponse
    
    # ì›¹í›… í˜¸ì¶œ ë¡œê¹… ì¶”ê°€
    logging.info("=" * 50)
    logging.info("[ğŸ”” ì›¹í›… í˜¸ì¶œë¨] Token ë¶„ì„ ê²°ê³¼ ì›¹í›… ìˆ˜ì‹ ")
    logging.info(f"[ì›¹í›… ìš”ì²­ IP] {request.client.host if request.client else 'Unknown'}")
    logging.info(f"[ì›¹í›… í—¤ë”] {dict(request.headers)}")
    
    job_id = request.query_params.get("job_id")
    task_id = request.query_params.get("task_id")
    
    logging.info(f"[ì›¹í›… íŒŒë¼ë¯¸í„°] job_id={job_id}, task_id={task_id}")

    if not job_id:
        logging.warning("[â—ê²½ê³ ] job_id ì—†ì´ webhook ë„ì°©. ë¬´ì‹œë¨")
        return JSONResponse(status_code=400, content={"error": "job_id is required"})

    # ì´ë¯¸ ì™„ë£Œëœ job_idì¸ì§€ í™•ì¸
    existing_result = get_analysis_result(db, job_id)
    if existing_result and existing_result.status == "completed":
        logging.info(f"[ì¤‘ë³µ ìš”ì²­ ë¬´ì‹œ] job_id={job_id}ëŠ” ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ")
        return {"received": True, "job_id": job_id, "message": "ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…"}

    data = await request.json()
    # analysis_results í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„, ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©
    if "analysis_results" in data:
        results = data.get("analysis_results", {})
    else:
        results = data  # ì „ì²´ ë°ì´í„°ë¥¼ ê²°ê³¼ë¡œ ì‚¬ìš©
    
    logging.info(f"[ì›¹í›… ë°ì´í„°] ë°›ì€ ê²°ê³¼ í¬ê¸°: {len(str(results))} ë¬¸ì")
    logging.info(f"[ì›¹í›… ë°ì´í„°] ê²°ê³¼ í‚¤ë“¤: {list(results.keys()) if isinstance(results, dict) else 'Not dict'}")
    
    # ë¶„ì„ ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
    update_analysis_result(db, job_id, 
                          status="completed", 
                          progress=100, 
                          result=results, 
                          message="ë¶„ì„ ì™„ë£Œ")

    logging.info(f"[âœ… ì›¹í›… ì²˜ë¦¬ ì™„ë£Œ] job_id={job_id}, task_id={task_id}")
    logging.info("=" * 50)
    return {"received": True, "job_id": job_id, "task_id": task_id}


# 3. í´ë¼ì´ì–¸íŠ¸ê°€ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ê²°ê³¼ ì¡°íšŒ API
@router.get("/analysis-result/{job_id}")
def get_analysis_result_api(job_id: str, db: Session = Depends(get_db)):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    stored_data = get_analysis_result(db, job_id)
    if not stored_data:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {
        "job_id": stored_data.job_id,
        "token_id": stored_data.token_id,
        "status": stored_data.status,
        "progress": stored_data.progress,
        "result": stored_data.result,
        "message": stored_data.message,
        "created_at": stored_data.created_at
    }


# 4. ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ ìœ„í•œ Server-Sent Events ì—”ë“œí¬ì¸íŠ¸
@router.get("/analysis-progress/{job_id}")
async def stream_analysis_progress(job_id: str):
    """ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë° (SSE)"""
    async def event_generator():
        from database import SessionLocal
        
        while True:
            try:
                # ìƒˆë¡œìš´ DB ì„¸ì…˜ìœ¼ë¡œ í˜„ì¬ ìƒíƒœ ì¡°íšŒ
                db = SessionLocal()
                try:
                    current_data = get_analysis_result(db, job_id)
                    if not current_data:
                        yield f"data: {json.dumps({'error': 'Job not found'}, ensure_ascii=False)}\n\n"
                        break
                    
                    data_dict = {
                        "job_id": current_data.job_id,
                        "status": current_data.status,
                        "progress": current_data.progress,
                        "message": current_data.message,
                        "result": current_data.result
                    }
                    
                    # ì™„ë£Œëœ ê²½ìš° ë§ˆì§€ë§‰ ë°ì´í„° ì „ì†¡ í›„ ì¢…ë£Œ
                    if current_data.status in ["completed", "failed"]:
                        yield f"data: {json.dumps(data_dict, ensure_ascii=False)}\n\n"
                        break
                    
                    yield f"data: {json.dumps(data_dict, ensure_ascii=False)}\n\n"
                finally:
                    db.close()
                    
                await asyncio.sleep(3)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                
            except Exception as e:
                logging.error(f"SSE ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
                error_data = {
                    "status": "error", 
                    "error": str(e),
                    "job_id": job_id
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                break
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )